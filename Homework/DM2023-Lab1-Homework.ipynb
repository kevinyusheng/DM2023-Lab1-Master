{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDNTc1wFid-1"
   },
   "source": [
    "### Student Information\n",
    "Name:林育陞\n",
    "\n",
    "Student ID: 112034515\n",
    "\n",
    "GitHub ID:  kevinyusheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BajKEF_id-5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSzgpPbVid-6"
   },
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjbdfsDcid-6"
   },
   "source": [
    "1. First: do the **take home** exercises in the [DM2023-Lab1-Master](https://github.com/fjrialdnc0615/DM2023-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2023-Lab1-Master](https://github.com/fjrialdnc0615/DM2023-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. You need to combine three labeled datasets into one file for your data preparation part.\n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fjrialdnc0615/DM2023-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Thursday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:14:42.474184Z",
     "start_time": "2023-10-20T19:14:42.442379Z"
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1666241724343,
     "user": {
      "displayName": "Kevin Yang",
      "userId": "04518680380941289042"
     },
     "user_tz": -480
    },
    "id": "IDH6foBIid-9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:14:43.626620Z",
     "start_time": "2023-10-20T19:14:43.435834Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_amazon= pd.read_table('./sentiment labelled sentences/amazon_cells_labelled.txt',header=None)\n",
    "df_imdb= pd.read_table('./sentiment labelled sentences/imdb_labelled.txt',header=None)\n",
    "df_yelp= pd.read_table('./sentiment labelled sentences/yelp_labelled.txt',header=None)\n",
    "df_amazon.columns = df_imdb.columns = df_yelp.columns = [\"text\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:14:49.937101Z",
     "start_time": "2023-10-20T19:14:49.897538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.iloc[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:14:52.295289Z",
     "start_time": "2023-10-20T19:14:52.262239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great\n",
      "for\n",
      "the\n",
      "jawbone.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(df_amazon.loc[2,'text'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:14:52.585629Z",
     "start_time": "2023-10-20T19:14:52.549979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon data volume :  1000\n",
      "imbd data volume :  748\n",
      "yelp data volume :  1000\n"
     ]
    }
   ],
   "source": [
    "print(\"amazon data volume : \" ,len(df_amazon))\n",
    "print(\"imbd data volume : \" ,len(df_imdb))\n",
    "print(\"yelp data volume : \" ,len(df_yelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:14:52.992449Z",
     "start_time": "2023-10-20T19:14:52.939944Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_amazon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mdata_amazon\u001b[49m,data_imdb,data_yelp],\n\u001b[1;32m      2\u001b[0m                    axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m df_all\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_amazon' is not defined"
     ]
    }
   ],
   "source": [
    "df_all = pd.concat([data_amazon,data_imdb,data_yelp],\n",
    "                   axis=0, ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DM with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T19:17:08.203494Z",
     "start_time": "2023-10-20T19:17:08.158808Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers.data_mining_helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# my functions\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_mining_helpers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdmh\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: dmh\u001b[38;5;241m.\u001b[39mcheck_missing_values(x))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers.data_mining_helpers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "df.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PQPCUbx1ie4R",
    "8qg4up1B_EhD",
    "lC7ymUlG_fai",
    "xtvWLH1x_7nV",
    "bgULadKFBXL-",
    "SZ4rgA1mBir5",
    "9VirxMl6CGN2",
    "yoTS9Vh8ESzB",
    "NGVM3wSjFt7v",
    "bH9BQLSWF9Uf",
    "y8I6L8Z8JGsv",
    "TFIl1hpMJqnv",
    "DobYRQ4FLetu",
    "9pfemrkcLiUG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
